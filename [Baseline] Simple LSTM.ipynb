{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2bcba5f-002e-4f49-9622-ada6117faf0a",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b0d9b68-7102-4eca-9543-3b9b8acafc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d13862e3-bb27-47af-9b58-a9fbf804df71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7df3f2-62d0-4499-a46e-47d01699def0",
   "metadata": {},
   "source": [
    "## Hyperparameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3367399-9798-4e38-967b-fd2320b9a2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'EPOCHS':100,\n",
    "    'LEARNING_RATE':1e-3,\n",
    "    'BATCH_SIZE':64,\n",
    "    'SEED':41\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4254e860-ff82-43ba-bfa3-fcee4eb3ddbd",
   "metadata": {},
   "source": [
    "## Fixed RandomSeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "101a714b-71b6-4475-a4ce-fa5f98bc2731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a4172e-5791-446f-9616-35c09d8bf25a",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a62c78cd-4f40-4e98-b8a6-1b6f1d906b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input_list = sorted(glob.glob('./data/train_input/*.csv'))\n",
    "all_target_list = sorted(glob.glob('./data/train_target/*.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a99a2ef1-ba9d-45b1-9581-0bcc82e96b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_list = all_input_list[:25]\n",
    "train_target_list = all_target_list[:25]\n",
    "\n",
    "val_input_list = all_input_list[25:]\n",
    "val_target_list = all_target_list[25:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac27ed36-8031-47a7-bd0d-a913513f2e8e",
   "metadata": {},
   "source": [
    "## CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16fd60a5-24e2-4539-bfd0-1c374a641699",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, input_paths, target_paths, infer_mode):\n",
    "        self.input_paths = input_paths\n",
    "        self.target_paths = target_paths\n",
    "        self.infer_mode = infer_mode\n",
    "        \n",
    "        self.data_list = []\n",
    "        self.label_list = []\n",
    "        print('Data Pre-processing..')\n",
    "        for input_path, target_path in tqdm(zip(self.input_paths, self.target_paths)):\n",
    "            input_df = pd.read_csv(input_path)\n",
    "            target_df = pd.read_csv(target_path)\n",
    "            \n",
    "            input_df = input_df.drop(columns=['obs_time'])\n",
    "            input_df = input_df.fillna(method='ffill')\n",
    "            \n",
    "            input_length = int(len(input_df)/24)\n",
    "            target_length = int(len(target_df))\n",
    "            \n",
    "            for idx in range(target_length):\n",
    "                time_series = input_df[24*idx:24*(idx+1)].values\n",
    "                self.data_list.append(torch.Tensor(time_series))\n",
    "            for label in target_df[\"predicted_weight_g\"]:\n",
    "                self.label_list.append(label)\n",
    "        print('Done.')\n",
    "              \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data_list[idx]\n",
    "        label = self.label_list[idx]\n",
    "        if self.infer_mode == False:\n",
    "            return data, label\n",
    "        else:\n",
    "            return data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d880481-1965-499d-9caa-fdfa8526f789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Pre-processing..\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c2ea8bc09d146d8acfb914c853882b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Data Pre-processing..\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeb1b500ca9b476890d9f5c13cd6ac46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CustomDataset(train_input_list, train_target_list, False)\n",
    "train_loader = DataLoader(train_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False)\n",
    "\n",
    "val_dataset = CustomDataset(val_input_list, val_target_list, False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39962463-032f-490a-a76d-c03991795f38",
   "metadata": {},
   "source": [
    "## Model Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9bb6aa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1d_LSTM(nn.Module):\n",
    "    def __init__(self, in_channel=15, out_channel=1):\n",
    "        super(Conv1d_LSTM, self).__init__()\n",
    "        self.conv2d_1 = nn.Conv1d(in_channels=in_channel,\n",
    "                                out_channels=64,\n",
    "                                kernel_size=1,\n",
    "                                stride=1,\n",
    "                                padding=1)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=64,\n",
    "                    hidden_size=32,\n",
    "                    num_layers=1,\n",
    "                    bias=True,\n",
    "                    bidirectional=False,\n",
    "                    batch_first=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.dense1 = nn.Linear(32, 16)\n",
    "        self.dense2 = nn.Linear(16, out_channel)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.conv2d_1(x)\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        self.lstm.flatten_parameters()\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        \n",
    "        x = hidden[-1]\n",
    "                \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.dense1(x)\n",
    "\n",
    "        x = self.dense2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122af0aa-a1fd-4595-9488-35761e3cb596",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a17df6b3-16c9-44dd-b0fd-ffb501fee749",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, val_loader, scheduler, device):\n",
    "    model.to(device)\n",
    "    criterion = nn.L1Loss().to(device)\n",
    "    \n",
    "    best_loss = 9999\n",
    "    best_model = None\n",
    "    for epoch in range(1, CFG['EPOCHS']+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for X, Y in iter(train_loader):\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(X)\n",
    "            loss = criterion(output, Y)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "                    \n",
    "        val_loss = validation(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f'Train Loss : [{np.mean(train_loss):.5f}] Valid Loss : [{val_loss:.5f}]')\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step(val_loss)\n",
    "            \n",
    "        if best_loss > val_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = model\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a24d422f-6e6d-4659-a6f8-c17e7f6761ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    with torch.no_grad():\n",
    "        for X, Y in iter(val_loader):\n",
    "            X = X.float().to(device)\n",
    "            Y = Y.float().to(device)\n",
    "            \n",
    "            model_pred = model(X)\n",
    "            loss = criterion(model_pred, Y)\n",
    "            \n",
    "            val_loss.append(loss.item())\n",
    "            \n",
    "    return np.mean(val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51da39f9-904f-4abd-a7d2-cdf29c4a6c24",
   "metadata": {},
   "source": [
    "## Run!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "86142d9a-68b7-4d04-8423-49d28025411d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : [35.86731] Valid Loss : [27.19976]\n",
      "Train Loss : [35.58037] Valid Loss : [26.81918]\n",
      "Train Loss : [35.22353] Valid Loss : [26.36846]\n",
      "Train Loss : [34.71312] Valid Loss : [25.65291]\n",
      "Train Loss : [34.04674] Valid Loss : [24.80718]\n",
      "Train Loss : [33.32930] Valid Loss : [23.88054]\n",
      "Train Loss : [32.65812] Valid Loss : [22.99405]\n",
      "Train Loss : [32.02459] Valid Loss : [22.16323]\n",
      "Train Loss : [31.47655] Valid Loss : [21.54675]\n",
      "Train Loss : [31.12885] Valid Loss : [21.11747]\n",
      "Train Loss : [30.90630] Valid Loss : [20.80804]\n",
      "Train Loss : [30.74224] Valid Loss : [20.68713]\n",
      "Train Loss : [30.65576] Valid Loss : [20.61042]\n",
      "Train Loss : [30.58052] Valid Loss : [20.55400]\n",
      "Train Loss : [30.50250] Valid Loss : [20.54093]\n",
      "Train Loss : [30.51318] Valid Loss : [20.56458]\n",
      "Train Loss : [30.46329] Valid Loss : [20.57231]\n",
      "Train Loss : [30.42005] Valid Loss : [20.57471]\n",
      "Epoch 00018: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Train Loss : [30.45558] Valid Loss : [20.58967]\n",
      "Train Loss : [30.51097] Valid Loss : [20.57772]\n",
      "Train Loss : [30.44423] Valid Loss : [20.58761]\n",
      "Epoch 00021: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Train Loss : [30.41155] Valid Loss : [20.59107]\n",
      "Train Loss : [30.34694] Valid Loss : [20.58355]\n",
      "Train Loss : [30.39702] Valid Loss : [20.59020]\n",
      "Epoch 00024: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Train Loss : [30.37278] Valid Loss : [20.58971]\n",
      "Train Loss : [30.35717] Valid Loss : [20.58503]\n",
      "Train Loss : [30.37918] Valid Loss : [20.55960]\n",
      "Epoch 00027: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Train Loss : [30.37823] Valid Loss : [20.55895]\n",
      "Train Loss : [30.35872] Valid Loss : [20.55534]\n",
      "Train Loss : [30.36263] Valid Loss : [20.56551]\n",
      "Epoch 00030: reducing learning rate of group 0 to 3.1250e-05.\n",
      "Train Loss : [30.34892] Valid Loss : [20.56576]\n",
      "Train Loss : [30.32567] Valid Loss : [20.56620]\n",
      "Train Loss : [30.29685] Valid Loss : [20.56079]\n",
      "Epoch 00033: reducing learning rate of group 0 to 1.5625e-05.\n",
      "Train Loss : [30.38293] Valid Loss : [20.56014]\n",
      "Train Loss : [30.37369] Valid Loss : [20.55972]\n",
      "Train Loss : [30.37386] Valid Loss : [20.56034]\n",
      "Epoch 00036: reducing learning rate of group 0 to 7.8125e-06.\n",
      "Train Loss : [30.38130] Valid Loss : [20.56053]\n",
      "Train Loss : [30.36980] Valid Loss : [20.56052]\n",
      "Train Loss : [30.32297] Valid Loss : [20.56066]\n",
      "Epoch 00039: reducing learning rate of group 0 to 3.9063e-06.\n",
      "Train Loss : [30.41773] Valid Loss : [20.56061]\n",
      "Train Loss : [30.33949] Valid Loss : [20.56061]\n",
      "Train Loss : [30.29770] Valid Loss : [20.56082]\n",
      "Epoch 00042: reducing learning rate of group 0 to 1.9531e-06.\n",
      "Train Loss : [30.38761] Valid Loss : [20.56083]\n",
      "Train Loss : [30.33074] Valid Loss : [20.56053]\n",
      "Train Loss : [30.37294] Valid Loss : [20.56057]\n",
      "Epoch 00045: reducing learning rate of group 0 to 9.7656e-07.\n",
      "Train Loss : [30.33610] Valid Loss : [20.56056]\n",
      "Train Loss : [30.34298] Valid Loss : [20.56055]\n",
      "Train Loss : [30.36026] Valid Loss : [20.56047]\n",
      "Epoch 00048: reducing learning rate of group 0 to 4.8828e-07.\n",
      "Train Loss : [30.39534] Valid Loss : [20.56051]\n",
      "Train Loss : [30.31298] Valid Loss : [20.56054]\n",
      "Train Loss : [30.34868] Valid Loss : [20.56053]\n",
      "Epoch 00051: reducing learning rate of group 0 to 2.4414e-07.\n",
      "Train Loss : [30.35166] Valid Loss : [20.56052]\n",
      "Train Loss : [30.31998] Valid Loss : [20.56052]\n",
      "Train Loss : [30.36534] Valid Loss : [20.56052]\n",
      "Epoch 00054: reducing learning rate of group 0 to 1.2207e-07.\n",
      "Train Loss : [30.35855] Valid Loss : [20.56052]\n",
      "Train Loss : [30.33285] Valid Loss : [20.56052]\n",
      "Train Loss : [30.38610] Valid Loss : [20.56052]\n",
      "Epoch 00057: reducing learning rate of group 0 to 6.1035e-08.\n",
      "Train Loss : [30.34373] Valid Loss : [20.56052]\n",
      "Train Loss : [30.37391] Valid Loss : [20.56052]\n",
      "Train Loss : [30.35344] Valid Loss : [20.56051]\n",
      "Epoch 00060: reducing learning rate of group 0 to 3.0518e-08.\n",
      "Train Loss : [30.39437] Valid Loss : [20.56051]\n",
      "Train Loss : [30.34369] Valid Loss : [20.56051]\n",
      "Train Loss : [30.40251] Valid Loss : [20.56051]\n",
      "Epoch 00063: reducing learning rate of group 0 to 1.5259e-08.\n",
      "Train Loss : [30.35165] Valid Loss : [20.56051]\n",
      "Train Loss : [30.34058] Valid Loss : [20.56051]\n",
      "Train Loss : [30.33645] Valid Loss : [20.56051]\n",
      "Train Loss : [30.37771] Valid Loss : [20.56051]\n",
      "Train Loss : [30.37895] Valid Loss : [20.56051]\n",
      "Train Loss : [30.38608] Valid Loss : [20.56051]\n",
      "Train Loss : [30.34183] Valid Loss : [20.56051]\n",
      "Train Loss : [30.40461] Valid Loss : [20.56051]\n",
      "Train Loss : [30.33769] Valid Loss : [20.56051]\n",
      "Train Loss : [30.37523] Valid Loss : [20.56051]\n",
      "Train Loss : [30.33702] Valid Loss : [20.56051]\n",
      "Train Loss : [30.30746] Valid Loss : [20.56051]\n",
      "Train Loss : [30.32604] Valid Loss : [20.56051]\n",
      "Train Loss : [30.34380] Valid Loss : [20.56051]\n",
      "Train Loss : [30.33880] Valid Loss : [20.56051]\n",
      "Train Loss : [30.39459] Valid Loss : [20.56046]\n",
      "Train Loss : [30.35735] Valid Loss : [20.56046]\n",
      "Train Loss : [30.33418] Valid Loss : [20.56046]\n",
      "Train Loss : [30.28683] Valid Loss : [20.56046]\n",
      "Train Loss : [30.32114] Valid Loss : [20.56046]\n",
      "Train Loss : [30.33864] Valid Loss : [20.56046]\n",
      "Train Loss : [30.34640] Valid Loss : [20.56046]\n",
      "Train Loss : [30.37447] Valid Loss : [20.56046]\n",
      "Train Loss : [30.37296] Valid Loss : [20.56046]\n",
      "Train Loss : [30.36882] Valid Loss : [20.56046]\n",
      "Train Loss : [30.33373] Valid Loss : [20.56046]\n",
      "Train Loss : [30.34402] Valid Loss : [20.56046]\n",
      "Train Loss : [30.32852] Valid Loss : [20.56046]\n",
      "Train Loss : [30.37979] Valid Loss : [20.56046]\n",
      "Train Loss : [30.34302] Valid Loss : [20.56046]\n",
      "Train Loss : [30.37501] Valid Loss : [20.56046]\n",
      "Train Loss : [30.33372] Valid Loss : [20.56046]\n",
      "Train Loss : [30.37918] Valid Loss : [20.56046]\n",
      "Train Loss : [30.35037] Valid Loss : [20.56046]\n",
      "Train Loss : [30.36849] Valid Loss : [20.56046]\n",
      "Train Loss : [30.35094] Valid Loss : [20.56046]\n",
      "Train Loss : [30.38252] Valid Loss : [20.56046]\n"
     ]
    }
   ],
   "source": [
    "model = Conv1d_LSTM()\n",
    "model.eval()\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, threshold_mode='abs',min_lr=1e-8, verbose=True)\n",
    "\n",
    "best_model = train(model, optimizer, train_loader, val_loader, scheduler, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93c88c8-95f2-4eae-a9ff-c81becba0d97",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f46d7d60-38d7-44d6-82f2-836738b5a85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_list = sorted(glob.glob('./test_input/*.csv'))\n",
    "test_target_list = sorted(glob.glob('./test_target/*.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad1defba-fdc0-4fe4-8c59-36d338851f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_per_case(model, test_loader, test_path, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    pred_list = []\n",
    "    with torch.no_grad():\n",
    "        for X in iter(test_loader):\n",
    "            X = X.float().to(device)\n",
    "            \n",
    "            model_pred = model(X)\n",
    "            \n",
    "            model_pred = model_pred.cpu().numpy().reshape(-1).tolist()\n",
    "            \n",
    "            pred_list += model_pred\n",
    "    \n",
    "    submit_df = pd.read_csv(test_path)\n",
    "    submit_df['predicted_weight_g'] = pred_list\n",
    "    submit_df.to_csv(test_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c88e68cb-dec1-439d-9363-03b817230bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Pre-processing..\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27e863a41f2b45619e12fb01f1f4eb9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Data Pre-processing..\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f13de170f8a64e7f98c2c53be43d21d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Data Pre-processing..\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf4639a98b3a4529a31be3fcaed5160a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Data Pre-processing..\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13d064c281d04756a88fa7f4e7550394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Data Pre-processing..\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c145a462498d40cfa3d8751d8d41b67d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "for test_input_path, test_target_path in zip(test_input_list, test_target_list):\n",
    "    test_dataset = CustomDataset([test_input_path], [test_target_path], True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=0)\n",
    "    inference_per_case(best_model, test_loader, test_target_path, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae208a0-548d-4af6-9798-0e247543b301",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "173e281a-7a9f-4878-b406-4419698f7e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "os.chdir(\"./test_target/\")\n",
    "submission = zipfile.ZipFile(\"../submission.zip\", 'w')\n",
    "for path in test_target_list:\n",
    "    path = path.split('/')[-1]\n",
    "    submission.write(path)\n",
    "submission.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "b081a66ee97bd2b6a16f43955f1d810b7ea816d6eaeb65e157ef9e038445f0c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
