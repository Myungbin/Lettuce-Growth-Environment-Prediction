{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2bcba5f-002e-4f49-9622-ada6117faf0a",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2b0d9b68-7102-4eca-9543-3b9b8acafc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d13862e3-bb27-47af-9b58-a9fbf804df71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7df3f2-62d0-4499-a46e-47d01699def0",
   "metadata": {},
   "source": [
    "## Hyperparameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c3367399-9798-4e38-967b-fd2320b9a2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'EPOCHS':1000,\n",
    "    'LEARNING_RATE':1e-3,\n",
    "    'BATCH_SIZE':64,\n",
    "    'SEED':41\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4254e860-ff82-43ba-bfa3-fcee4eb3ddbd",
   "metadata": {},
   "source": [
    "## Fixed RandomSeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "101a714b-71b6-4475-a4ce-fa5f98bc2731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a4172e-5791-446f-9616-35c09d8bf25a",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a62c78cd-4f40-4e98-b8a6-1b6f1d906b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input_list = sorted(glob.glob('./data/train_input/*.csv'))\n",
    "all_target_list = sorted(glob.glob('./data/train_target/*.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b5688445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_input_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a99a2ef1-ba9d-45b1-9581-0bcc82e96b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_list = all_input_list[:25]\n",
    "train_target_list = all_target_list[:25]\n",
    "\n",
    "val_input_list = all_input_list[25:]\n",
    "val_target_list = all_target_list[25:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac27ed36-8031-47a7-bd0d-a913513f2e8e",
   "metadata": {},
   "source": [
    "## CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "16fd60a5-24e2-4539-bfd0-1c374a641699",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, input_paths, target_paths, infer_mode):\n",
    "        self.input_paths = input_paths\n",
    "        self.target_paths = target_paths\n",
    "        self.infer_mode = infer_mode\n",
    "        \n",
    "        self.data_list = []\n",
    "        self.label_list = []\n",
    "        print('Data Pre-processing..')\n",
    "        for input_path, target_path in tqdm(zip(self.input_paths, self.target_paths)):\n",
    "            input_df = pd.read_csv(input_path)\n",
    "            target_df = pd.read_csv(target_path)\n",
    "            \n",
    "            input_df = input_df.drop(columns=['obs_time'])\n",
    "            input_df = input_df.fillna(method='ffill')\n",
    "            \n",
    "            input_length = int(len(input_df)/24)\n",
    "            target_length = int(len(target_df))\n",
    "            \n",
    "            for idx in range(target_length):\n",
    "                time_series = input_df[24*idx:24*(idx+1)].values\n",
    "                self.data_list.append(torch.Tensor(time_series))\n",
    "            for label in target_df[\"predicted_weight_g\"]:\n",
    "                self.label_list.append(label)\n",
    "        print('Done.')\n",
    "              \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data_list[idx]\n",
    "        label = self.label_list[idx]\n",
    "        if self.infer_mode == False:\n",
    "            return data, label\n",
    "        else:\n",
    "            return data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9d880481-1965-499d-9caa-fdfa8526f789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Pre-processing..\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bc2f48187e742fc97481039138a0866",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Data Pre-processing..\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55e3d6bbdedb4550ab208acdc6715321",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CustomDataset(train_input_list, train_target_list, False)\n",
    "train_loader = DataLoader(train_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False)\n",
    "\n",
    "val_dataset = CustomDataset(val_input_list, val_target_list, False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39962463-032f-490a-a76d-c03991795f38",
   "metadata": {},
   "source": [
    "## Model Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9bb6aa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1d_LSTM(nn.Module):\n",
    "    def __init__(self, in_channel=15, out_channel=1):\n",
    "        super(Conv1d_LSTM, self).__init__()\n",
    "        self.conv2d_1 = nn.Conv1d(in_channels=in_channel,\n",
    "                                out_channels=64,\n",
    "                                kernel_size=1,\n",
    "                                stride=1,\n",
    "                                padding=1)\n",
    "        \n",
    "        self.maxpool = nn.MaxPool1d(10, stride=1)\n",
    "        self.lstm = nn.LSTM(input_size=64,\n",
    "                    hidden_size=32,\n",
    "                    num_layers=1,\n",
    "                    bias=True,\n",
    "                    bidirectional=False,\n",
    "                    batch_first=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.dense1 = nn.Linear(32, 16)\n",
    "        self.dense2 = nn.Linear(16, out_channel)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.conv2d_1(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        self.lstm.flatten_parameters()\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        \n",
    "        x = hidden[-1]\n",
    "        \n",
    "        x = self.tanh(x)        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.dense1(x)\n",
    "\n",
    "        x = self.dense2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122af0aa-a1fd-4595-9488-35761e3cb596",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a17df6b3-16c9-44dd-b0fd-ffb501fee749",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, val_loader, scheduler, device):\n",
    "    model.to(device)\n",
    "    criterion = nn.L1Loss().to(device)\n",
    "    \n",
    "    best_loss = 9999\n",
    "    best_model = None\n",
    "    for epoch in range(1, CFG['EPOCHS']+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for X, Y in iter(train_loader):\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(X)\n",
    "            loss = criterion(output, Y)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "                    \n",
    "        val_loss = validation(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f'Train Loss : [{np.mean(train_loss):.5f}] Valid Loss : [{val_loss:.5f}]')\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step(val_loss)\n",
    "            \n",
    "        if best_loss > val_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = model\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a24d422f-6e6d-4659-a6f8-c17e7f6761ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    with torch.no_grad():\n",
    "        for X, Y in iter(val_loader):\n",
    "            X = X.float().to(device)\n",
    "            Y = Y.float().to(device)\n",
    "            \n",
    "            model_pred = model(X)\n",
    "            loss = criterion(model_pred, Y)\n",
    "            \n",
    "            val_loss.append(loss.item())\n",
    "            \n",
    "    return np.mean(val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51da39f9-904f-4abd-a7d2-cdf29c4a6c24",
   "metadata": {},
   "source": [
    "## Run!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "86142d9a-68b7-4d04-8423-49d28025411d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Epoch 00003: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Epoch 00006: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Epoch 00009: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Epoch 00012: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Epoch 00015: reducing learning rate of group 0 to 3.1250e-05.\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Epoch 00018: reducing learning rate of group 0 to 1.5625e-05.\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Epoch 00021: reducing learning rate of group 0 to 7.8125e-06.\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Epoch 00024: reducing learning rate of group 0 to 3.9063e-06.\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Epoch 00027: reducing learning rate of group 0 to 1.9531e-06.\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Epoch 00030: reducing learning rate of group 0 to 9.7656e-07.\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Epoch 00033: reducing learning rate of group 0 to 4.8828e-07.\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Epoch 00036: reducing learning rate of group 0 to 2.4414e-07.\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Epoch 00039: reducing learning rate of group 0 to 1.2207e-07.\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Epoch 00042: reducing learning rate of group 0 to 6.1035e-08.\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Epoch 00045: reducing learning rate of group 0 to 3.0518e-08.\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Epoch 00048: reducing learning rate of group 0 to 1.5259e-08.\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n",
      "Train Loss : [nan] Valid Loss : [nan]\n"
     ]
    }
   ],
   "source": [
    "model = Conv1d_LSTM()\n",
    "model.eval()\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, threshold_mode='abs',min_lr=1e-8, verbose=True)\n",
    "\n",
    "best_model = train(model, optimizer, train_loader, val_loader, scheduler, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93c88c8-95f2-4eae-a9ff-c81becba0d97",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f46d7d60-38d7-44d6-82f2-836738b5a85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_list = sorted(glob.glob('./data/test_input/*.csv'))\n",
    "test_target_list = sorted(glob.glob('./data/test_target/*.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ad1defba-fdc0-4fe4-8c59-36d338851f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_per_case(model, test_loader, test_path, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    pred_list = []\n",
    "    with torch.no_grad():\n",
    "        for X in iter(test_loader):\n",
    "            X = X.float().to(device)\n",
    "            \n",
    "            model_pred = model(X)\n",
    "            \n",
    "            model_pred = model_pred.cpu().numpy().reshape(-1).tolist()\n",
    "            \n",
    "            pred_list += model_pred\n",
    "    \n",
    "    submit_df = pd.read_csv(test_path)\n",
    "    submit_df['predicted_weight_g'] = pred_list\n",
    "    submit_df.to_csv(test_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c88e68cb-dec1-439d-9363-03b817230bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_input_path, test_target_path in zip(test_input_list, test_target_list):\n",
    "    test_dataset = CustomDataset([test_input_path], [test_target_path], True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=0)\n",
    "    inference_per_case(best_model, test_loader, test_target_path, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae208a0-548d-4af6-9798-0e247543b301",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "173e281a-7a9f-4878-b406-4419698f7e0c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] 지정된 파일을 찾을 수 없습니다: './test_target/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\mb_job\\project\\Competition\\Lettuce-Growth-Environment-Prediction\\[Baseline] Simple LSTM.ipynb 셀 26\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/mb_job/project/Competition/Lettuce-Growth-Environment-Prediction/%5BBaseline%5D%20Simple%20LSTM.ipynb#X34sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mzipfile\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/mb_job/project/Competition/Lettuce-Growth-Environment-Prediction/%5BBaseline%5D%20Simple%20LSTM.ipynb#X34sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m os\u001b[39m.\u001b[39;49mchdir(\u001b[39m\"\u001b[39;49m\u001b[39m./test_target/\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/mb_job/project/Competition/Lettuce-Growth-Environment-Prediction/%5BBaseline%5D%20Simple%20LSTM.ipynb#X34sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m submission \u001b[39m=\u001b[39m zipfile\u001b[39m.\u001b[39mZipFile(\u001b[39m\"\u001b[39m\u001b[39m../submission.zip\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/mb_job/project/Competition/Lettuce-Growth-Environment-Prediction/%5BBaseline%5D%20Simple%20LSTM.ipynb#X34sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m path \u001b[39min\u001b[39;00m test_target_list:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] 지정된 파일을 찾을 수 없습니다: './test_target/'"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "os.chdir(\"./test_target/\")\n",
    "submission = zipfile.ZipFile(\"../submission.zip\", 'w')\n",
    "for path in test_target_list:\n",
    "    path = path.split('/')[-1]\n",
    "    submission.write(path)\n",
    "submission.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c60bc1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "b081a66ee97bd2b6a16f43955f1d810b7ea816d6eaeb65e157ef9e038445f0c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
