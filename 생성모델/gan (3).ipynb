{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cygy69N84Gk"
      },
      "source": [
        "# grg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AS1qnZGD8smn",
        "outputId": "f4ce100a-2836-4969-efa3-fe304cb552e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Dec  8 04:10:58 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P0    27W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Rv-Bjso8skF",
        "outputId": "01393fbe-2e3c-475d-f893-a593ac6bcbc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6Fd1r4y8shy",
        "outputId": "edf8579b-6c78-4863-a0c7-288282faf16f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 13.6 gigabytes of available RAM\n",
            "\n",
            "Not using a high-RAM runtime\n"
          ]
        }
      ],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoM8KXCv82Er"
      },
      "source": [
        "# gan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wBRWQnVp-ifk"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "\n",
        "def preprocessing(input_path):\n",
        "    all_input_list = sorted(glob.glob(input_path))\n",
        "    train = pd.DataFrame()\n",
        "    for datapath in all_input_list:\n",
        "        data = pd.read_csv(datapath) \n",
        "  \n",
        "        data['obs_time'] = data.index % 24 \n",
        "        df = abs(data)\n",
        "        df.loc[(df['내부온도관측치'] > 40), '내부온도관측치'] = 40\n",
        "        df.loc[(df['내부습도관측치'] > 100), '내부습도관측치'] = 100\n",
        "        df.loc[(df['co2관측치'] > 1200), 'co2관측치'] = 1200\n",
        "        df.loc[(df['ec관측치'] > 8), 'ec관측치'] = 8\n",
        "        df.loc[(df['시간당분무량'] > 3000), '시간당분무량'] = 3000\n",
        "        df.loc[(df['시간당백색광량'] > 120000), '시간당백색광량'] = 120000\n",
        "        df.loc[(df['시간당적색광량'] > 120000), '시간당적색광량'] = 120000\n",
        "        df.loc[(df['시간당청색광량'] > 120000), '시간당청색광량'] = 120000\n",
        "        df.loc[(df['시간당총광량'] > 120000), '시간당총광량'] = 120000\n",
        "        df['시간당총광량'] = df['시간당청색광량']+df['시간당백색광량']+df['시간당적색광량']\n",
        "        \n",
        "        col_list = df.columns\n",
        "        for i in range(0,len(col_list)):\n",
        "            col = col_list[i]    \n",
        "            if '누적' in col : \n",
        "                df[col] = df.groupby((df.obs_time == 0).cumsum()).agg(col_list[i-1]).cumsum()   \n",
        "            df.to_csv(datapath,index=False)\n",
        "            train = pd.concat([train,df])\n",
        "    print('finish!!')\n",
        "    return train\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0djyR4b8-z7e",
        "outputId": "a27f2f8d-a5f5-425a-fc62-8fbbc69392e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "finish!!\n"
          ]
        }
      ],
      "source": [
        "traininput = preprocessing('drive/MyDrive/dacon/gan/상추/train_input/*.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "day0 = traininput[traininput['DAT']==0].reset_index(drop=True)\n",
        "day0 = day0[['obs_time','내부온도관측치','내부습도관측치','co2관측치','ec관측치','시간당분무량','시간당백색광량','시간당적색광량','시간당청색광량']]\n",
        "day0.describe()"
      ],
      "metadata": {
        "id": "ZwYZ31hm3-CV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "day0"
      ],
      "metadata": {
        "id": "tbY1c5aUzOWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "sc = MinMaxScaler()"
      ],
      "metadata": {
        "id": "ZccJOrAWiMCp"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc.fit(day0)\n",
        "newday0= sc.transform(day0)"
      ],
      "metadata": {
        "id": "_brVsVKuicbW"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import zeros\n",
        "from numpy import ones\n",
        "from numpy import hstack\n",
        "from numpy.random import rand\n",
        "from numpy.random import randn\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from matplotlib import pyplot"
      ],
      "metadata": {
        "id": "p2De1F5HFQRT"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.optim as optim \n",
        "from torch import nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "# 노이즈 -> 피쳐수만큼\n",
        "# generator input -> 노이즈가 들어감\n",
        "# generator ouput -> real data x 형태로 나와야 함.\n",
        "# discriminator input -> real data x 가 들어감\n",
        "# discriminator output -> 1\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Discriminator, self).__init__()\n",
        "    self.input = 9 \n",
        "    self.output= 1\n",
        "    self.model = nn.Sequential(\n",
        "        nn.Linear(self.input, 16), # input size, hidden size\n",
        "        nn.LeakyReLU(0.2),\n",
        "        nn.Dropout(0.2),\n",
        "        nn.Linear(16, 64),\n",
        "        nn.LeakyReLU(0.8),\n",
        "        nn.Linear(64, 128),\n",
        "        nn.LeakyReLU(0.5),\n",
        "        nn.Linear(128, self.output), # hidden size, output size\n",
        "        nn.Sigmoid()\n",
        "    ).to(device)\n",
        "\n",
        "  def forward(self, x):\n",
        "    #print(x.size())\n",
        "    x = self.model(x)\n",
        "    return x\n",
        "\n",
        "class Generator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Generator, self).__init__()\n",
        "    self.features = 9 # 피쳐수\n",
        "    self.output = 9  # 데이터수\n",
        "    self.model = nn.Sequential(\n",
        "        nn.Linear(self.features, 64), # input size, hidden size\n",
        "        nn.Sigmoid(),\n",
        "        nn.Dropout(0.2),\n",
        "        nn.Linear(64, 16),\n",
        "        #nn.LeakyReLU(0.2),\n",
        "        nn.Linear(16,self.output), # hidden size, output size\n",
        "        nn.Sigmoid()\n",
        "    ).to(device)\n",
        "\n",
        "  def forward(self, x):\n",
        "    #print('generator',x.size())\n",
        "    x = self.model(x)\n",
        "    return x\n",
        "\n",
        "# 모델 정의\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "generator = Generator().to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "g_optim = optim.Adam(generator.parameters(), lr=2e-4)\n",
        "d_optim = optim.Adam(discriminator.parameters(), lr=2e-4)\n",
        "\n",
        "criterion = nn.BCELoss().to(device)\n",
        "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer=g_optim, mode='min', verbose=True, patience=10, factor=0.5)\n",
        "\n",
        "import time\n",
        "n_epochs = 1000\n",
        "noise = 9\n",
        "start_time = time.time()\n",
        "newdata = []\n",
        "testdata = []\n",
        "#print(real_data)\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "  test = day0.iloc[:28].values\n",
        "  \n",
        "  nptonn = torch.from_numpy(newday0).float()\n",
        "\n",
        "  real = torch.cuda.FloatTensor(nptonn.size(0), 1).fill_(1.0) # \n",
        "  fake = torch.cuda.FloatTensor(nptonn.size(0), 1).fill_(0.0) # \n",
        "\n",
        "  real_data = nptonn.cuda()\n",
        "  g_optim.zero_grad()\n",
        "\n",
        "  z0 = torch.normal(mean=11.5, std=6.9, size=(nptonn.shape[0], 1)).cuda() # random noise\n",
        "  z1 = torch.normal(mean=25.78, std=4.3, size=(nptonn.shape[0], 1)).cuda() # random noise\n",
        "  z2 = torch.normal(mean=54.91, std=12.2, size=(nptonn.shape[0], 1)).cuda() # random noise\n",
        "  z3 = torch.normal(mean=533.833, std=144.1, size=(nptonn.shape[0], 1)).cuda() # random noise\n",
        "  z4 = torch.normal(mean=1.273, std=0.932, size=(nptonn.shape[0], 1)).cuda() # random noise\n",
        "  z5 = torch.normal(mean=430.600, std=491.308, size=(nptonn.shape[0], 1)).cuda() # random noise\n",
        "  z6 = torch.normal(mean=6765.408, std=9450.28, size=(nptonn.shape[0], 1)).cuda() # random noise\n",
        "  z7 = torch.normal(mean=1309.564, std=2653.722, size=(nptonn.shape[0], 1)).cuda() # random noise\n",
        "  z8 = torch.normal(mean=856.852, std=1938.17, size=(nptonn.shape[0], 1)).cuda() # random noise\n",
        "  zz = torch.normal(mean=0, std=1, size=(nptonn.shape[0], noise)).cuda()\n",
        "\n",
        "  z = torch.cat([z0,z1,z2,z3,z4,z5,z6,z7,z8], dim=1) #[M, N+N, K]\n",
        "  #print(z.size())\n",
        "\n",
        "  generated_dis = generator(z) # create distribution\n",
        "  #print(generated_dis.size())\n",
        "  #print(real_data.size())\n",
        "  generated_dis_value = generated_dis.detach().cpu()\n",
        "  g_loss =  criterion(discriminator(generated_dis), real) # calculate generator loss\n",
        "  \n",
        "  # update generator\n",
        "  g_loss.backward()\n",
        "  g_optim.step()\n",
        "\n",
        "  # update discriminator\n",
        "  real_loss = criterion(discriminator(real_data), real)\n",
        "  r_score = discriminator(real_data).mean()\n",
        "  fake_loss = criterion(discriminator(generated_dis.detach()), fake)\n",
        "  g_score = discriminator(generated_dis).mean()\n",
        "  d_loss = (real_loss + fake_loss) / 2\n",
        "\n",
        "  newdata.append(generated_dis_value)\n",
        "\n",
        "  d_loss.backward()\n",
        "  d_optim.step()\n",
        "\n",
        "  print(f\"[Epoch {epoch}/{n_epochs}] [D loss: {d_loss.item():.6f}] [G loss: {g_loss.item():.6f}] [FAKE loss: {fake_loss.item():.6f}] \",\n",
        "        g_score, r_score)"
      ],
      "metadata": {
        "id": "-QDx8n0j3zYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newdata[1][15]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VNqEGGHTf2g",
        "outputId": "953dcccb-cd81-4c3a-8909-80c209a8e246"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.4545, 0.4319, 0.5340, 0.5422, 0.4173, 0.5004, 0.4007, 0.5746, 0.4306])"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "newday0[15]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydcAhZJ6jNl2",
        "outputId": "8eebd577-89bd-4a49-ce8c-66df5242dd3f"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.65217391, 0.72686896, 0.92376881, 0.39502165, 0.45635057,\n",
              "       0.09213187, 0.15212658, 0.09833333, 0.        ])"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.optim as optim \n",
        "from torch import nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Discriminator, self).__init__()\n",
        "    self.input = 9 \n",
        "    self.output= 1\n",
        "    self.model = nn.Sequential(\n",
        "        nn.Linear(self.input, 32), # input size, hidden size\n",
        "        nn.LeakyReLU(0.5),\n",
        "        #nn.Dropout(0.2),\n",
        "        nn.Linear(32, 64),\n",
        "        nn.LeakyReLU(0.8),\n",
        "        nn.Linear(64, 128),\n",
        "        nn.LeakyReLU(0.5),\n",
        "        nn.Linear(128, self.output), # hidden size, output size\n",
        "        nn.Sigmoid()\n",
        "    ).to(device)\n",
        "\n",
        "  def forward(self, x):\n",
        "    #print(x.size())\n",
        "    x = self.model(x)\n",
        "    return x\n",
        "\n",
        "class Generator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Generator, self).__init__()\n",
        "    self.features = 9 # 피쳐수\n",
        "    self.output = 9  # 데이터수\n",
        "    self.model = nn.Sequential(\n",
        "        nn.Linear(self.features, 64), # input size, hidden size\n",
        "        nn.Sigmoid(),\n",
        "        nn.Dropout(0.2),\n",
        "        nn.Linear(64, 32),\n",
        "        #nn.LeakyReLU(0.2),\n",
        "        nn.Linear(32, self.output), # hidden size, output size\n",
        "        nn.Sigmoid()\n",
        "    ).to(device)\n",
        "\n",
        "  def forward(self, x):\n",
        "    #print('generator',x.size())\n",
        "    x = self.model(x)\n",
        "    return x\n",
        "\n",
        "# 모델 정의\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "generator = Generator().to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "g_optim = optim.Adam(generator.parameters(), lr=2e-4)\n",
        "d_optim = optim.Adam(discriminator.parameters(), lr=2e-4)\n",
        "\n",
        "criterion = nn.BCELoss().to(device)\n",
        "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer=g_optim, mode='min', verbose=True, patience=10, factor=0.5)\n",
        "\n",
        "import time\n",
        "n_epochs = 1000\n",
        "noise = 9\n",
        "start_time = time.time()\n",
        "newdata = []\n",
        "testdata = []\n",
        "#print(real_data)\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "  test = day0.iloc[:28].values\n",
        "  \n",
        "  nptonn = torch.from_numpy(newday0).float()\n",
        "\n",
        "  real = torch.cuda.FloatTensor(nptonn.size(0), 1).fill_(1.0) # \n",
        "  fake = torch.cuda.FloatTensor(nptonn.size(0), 1).fill_(0.0) # \n",
        "\n",
        "  real_data = nptonn.cuda()\n",
        "  g_optim.zero_grad()\n",
        "\n",
        "  z0 = torch.normal(mean=11.5, std=6.9, size=(nptonn.shape[0], 1)).cuda() # random noise\n",
        "  z1 = torch.normal(mean=25.78, std=4.3, size=(nptonn.shape[0], 1)).cuda() # random noise\n",
        "  z2 = torch.normal(mean=54.91, std=12.2, size=(nptonn.shape[0], 1)).cuda() # random noise\n",
        "  z3 = torch.normal(mean=533.833, std=144.1, size=(nptonn.shape[0], 1)).cuda() # random noise\n",
        "  z4 = torch.normal(mean=1.273, std=0.932, size=(nptonn.shape[0], 1)).cuda() # random noise\n",
        "  z5 = torch.normal(mean=430.600, std=491.308, size=(nptonn.shape[0], 1)).cuda() # random noise\n",
        "  z6 = torch.normal(mean=6765.408, std=9450.28, size=(nptonn.shape[0], 1)).cuda() # random noise\n",
        "  z7 = torch.normal(mean=1309.564, std=2653.722, size=(nptonn.shape[0], 1)).cuda() # random noise\n",
        "  z8 = torch.normal(mean=856.852, std=1938.17, size=(nptonn.shape[0], 1)).cuda() # random noise\n",
        "  zz = torch.normal(mean=0, std=1, size=(nptonn.shape[0], noise)).cuda()\n",
        "\n",
        "  z = torch.cat([z0,z1,z2,z3,z4,z5,z6,z7,z8], dim=1) #[M, N+N, K]\n",
        "  #print(z.size())\n",
        "\n",
        "  generated_dis = generator(z) # create distribution\n",
        "  #print(generated_dis.size())\n",
        "  #print(real_data.size())\n",
        "  generated_dis_value = generated_dis.detach().cpu()\n",
        "  g_loss =  criterion(discriminator(generated_dis), real) # calculate generator loss\n",
        "  \n",
        "  # update generator\n",
        "  g_loss.backward()\n",
        "  g_optim.step()\n",
        "\n",
        "  # update discriminator\n",
        "  real_loss = criterion(discriminator(real_data), real)\n",
        "  r_score = discriminator(real_data).mean()\n",
        "  fake_loss = criterion(discriminator(generated_dis.detach()), fake)\n",
        "  g_score = discriminator(generated_dis).mean()\n",
        "  d_loss = (real_loss + fake_loss) / 2\n",
        "\n",
        "  if g_score > 0.8 :\n",
        "    newdata.append(generated_dis_value)\n",
        "\n",
        "  d_loss.backward()\n",
        "  d_optim.step()\n",
        "\n",
        "  print(f\"[Epoch {epoch}/{n_epochs}] [D loss: {d_loss.item():.6f}] [G loss: {g_loss.item():.6f}] [FAKE loss: {fake_loss.item():.6f}] \",\n",
        "        g_score, r_score)"
      ],
      "metadata": {
        "id": "Udw2NdEwoM_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newdata[1][20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxGbIq8foM9G",
        "outputId": "4f8810ff-24fc-4b40-9702-b26da1098c10"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.6706, 0.5291, 0.4776, 0.7575, 0.4628, 0.7214, 0.5276, 0.3733, 0.4033])"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "newday0[20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m11B0YRNoM6D",
        "outputId": "6d9b6bb9-9582-4191-daf2-7f2d787511ec"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.86956522, 0.69230885, 0.97745217, 0.38405288, 0.45395526,\n",
              "       0.04371511, 0.00603349, 0.00466667, 0.        ])"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.optim as optim \n",
        "from torch import nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Discriminator, self).__init__()\n",
        "    self.input = 9 \n",
        "    self.output= 1\n",
        "    self.model = nn.Sequential(\n",
        "        nn.Linear(self.input, 32), # input size, hidden size\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.2),\n",
        "        nn.Linear(32, 64),\n",
        "        nn.LeakyReLU(0.8),\n",
        "        nn.Linear(64, 128),\n",
        "        nn.LeakyReLU(0.5),\n",
        "        nn.Linear(128, self.output), # hidden size, output size\n",
        "        nn.Sigmoid()\n",
        "    ).to(device)\n",
        "\n",
        "  def forward(self, x):\n",
        "    #print(x.size())\n",
        "    x = self.model(x)\n",
        "    return x\n",
        "\n",
        "class Generator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Generator, self).__init__()\n",
        "    self.features = 9 # 피쳐수\n",
        "    self.output = 9  # 데이터수\n",
        "    self.model = nn.Sequential(\n",
        "        nn.Linear(self.features, 128), # input size, hidden size\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.2),\n",
        "        nn.Linear(128, 32),\n",
        "        nn.Sigmoid(),\n",
        "        #nn.LeakyReLU(0.2),\n",
        "        nn.Linear(32, self.output), # hidden size, output size\n",
        "        nn.Sigmoid()\n",
        "    ).to(device)\n",
        "\n",
        "  def forward(self, x):\n",
        "    #print('generator',x.size())\n",
        "    x = self.model(x)\n",
        "    return x\n",
        "\n",
        "# 모델 정의\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "generator = Generator().to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "g_optim = optim.Adam(generator.parameters(), lr=2e-4)\n",
        "d_optim = optim.Adam(discriminator.parameters(), lr=2e-4)\n",
        "\n",
        "criterion = nn.BCELoss().to(device)\n",
        "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer=g_optim, mode='min', verbose=True, patience=10, factor=0.5)\n",
        "\n",
        "import time\n",
        "n_epochs = 1000\n",
        "noise = 9\n",
        "start_time = time.time()\n",
        "newdata = []\n",
        "testdata = []\n",
        "#print(real_data)\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "  test = day0.iloc[:28].values\n",
        "  \n",
        "  nptonn = torch.from_numpy(newday0).float()\n",
        "\n",
        "  real = torch.cuda.FloatTensor(nptonn.size(0), 1).fill_(1.0) # \n",
        "  fake = torch.cuda.FloatTensor(nptonn.size(0), 1).fill_(0.0) # \n",
        "\n",
        "  real_data = nptonn.cuda()\n",
        "  g_optim.zero_grad()\n",
        "\n",
        "  z0 = torch.normal(mean=11.5, std=6.9, size=(nptonn.shape[0], 1)).cuda() # random noise\n",
        "  z1 = torch.normal(mean=25.78, std=4.3, size=(nptonn.shape[0], 1)).cuda() # random noise\n",
        "  z2 = torch.normal(mean=54.91, std=12.2, size=(nptonn.shape[0], 1)).cuda() # random noise\n",
        "  z3 = torch.normal(mean=533.833, std=144.1, size=(nptonn.shape[0], 1)).cuda() # random noise\n",
        "  z4 = torch.normal(mean=1.273, std=0.932, size=(nptonn.shape[0], 1)).cuda() # random noise\n",
        "  z5 = torch.normal(mean=430.600, std=491.308, size=(nptonn.shape[0], 1)).cuda() # random noise\n",
        "  z6 = torch.normal(mean=6765.408, std=9450.28, size=(nptonn.shape[0], 1)).cuda() # random noise\n",
        "  z7 = torch.normal(mean=1309.564, std=2653.722, size=(nptonn.shape[0], 1)).cuda() # random noise\n",
        "  z8 = torch.normal(mean=856.852, std=1938.17, size=(nptonn.shape[0], 1)).cuda() # random noise\n",
        "  zz = torch.normal(mean=0, std=1, size=(nptonn.shape[0], noise)).cuda()\n",
        "\n",
        "  z = torch.cat([z0,z1,z2,z3,z4,z5,z6,z7,z8], dim=1) #[M, N+N, K]\n",
        "  #print(z.size())\n",
        "\n",
        "  generated_dis = generator(z) # create distribution\n",
        "  #print(generated_dis.size())\n",
        "  #print(real_data.size())\n",
        "  generated_dis_value = generated_dis.detach().cpu()\n",
        "  g_loss =  criterion(discriminator(generated_dis), real) # calculate generator loss\n",
        "  \n",
        "  # update generator\n",
        "  g_loss.backward()\n",
        "  g_optim.step()\n",
        "\n",
        "  # update discriminator\n",
        "  real_loss = criterion(discriminator(real_data), real)\n",
        "  r_score = discriminator(real_data).mean()\n",
        "  fake_loss = criterion(discriminator(generated_dis.detach()), fake)\n",
        "  g_score = discriminator(generated_dis).mean()\n",
        "  d_loss = (real_loss + fake_loss) / 2\n",
        "\n",
        "  if g_score > 0.8 :\n",
        "    newdata.append(generated_dis_value)\n",
        "\n",
        "  d_loss.backward()\n",
        "  d_optim.step()\n",
        "\n",
        "  print(f\"[Epoch {epoch}/{n_epochs}] [D loss: {d_loss.item():.6f}] [G loss: {g_loss.item():.6f}] [FAKE loss: {fake_loss.item():.6f}] \",\n",
        "        g_score, r_score)"
      ],
      "metadata": {
        "id": "u7sBFUQpoM3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newdata[1][2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrihcghOqMW2",
        "outputId": "1782e22f-d33b-4c55-b9dd-aa68df966b58"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.5201, 0.5828, 0.5117, 0.6643, 0.4171, 0.4135, 0.3808, 0.6583, 0.5251])"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dBn8CEg1qMR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ew2LdsnTqMOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W9g2WnYvqMKf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}